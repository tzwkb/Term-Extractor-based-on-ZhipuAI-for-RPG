"""
æœ¯è¯­æå–å·¥å…· - æ‰¹å¤„ç†å®ç°
æä¾›ä»æ–‡æœ¬ä¸­æå–ä¸“ä¸šæœ¯è¯­çš„åŠŸèƒ½ï¼Œä½¿ç”¨æ™ºè°±AIæ‰¹å¤„ç†API
"""

import os
import re
import json
import pandas as pd
import xlsxwriter
import requests
import time
import logging
from typing import List, Dict, Any, Optional, Tuple, Union
from pathlib import Path
import threading

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æ£€æŸ¥å¯é€‰ä¾èµ–
try:
    from zhipuai import ZhipuAI
    HAS_ZHIPUAI = True
except ImportError:
    HAS_ZHIPUAI = False
    logger.info("æœªå®‰è£…zhipuaiåº“ï¼Œæ— æ³•ä½¿ç”¨æ‰¹å¤„ç†åŠŸèƒ½")

# JSONå¤„ç†å·¥å…·å¯¼å…¥
try:
    from json_utils import fix_json_response, parse_json_safely, is_likely_json
    HAS_JSON_UTILS = True
except ImportError:
    HAS_JSON_UTILS = False
    logger.info("æœªå®‰è£…json_utilsæ¨¡å—ï¼Œå°†ä½¿ç”¨å¤‡ç”¨JSONå¤„ç†")

class TermExtractor:
    """æœ¯è¯­æå–å™¨æ ¸å¿ƒç±» - æ‰¹å¤„ç†æ¨¡å¼"""
    
    def __init__(self, api_key: Optional[str] = None, api_url: Optional[str] = None):
        """
        åˆå§‹åŒ–æœ¯è¯­æå–å™¨
        
        Args:
            api_key: APIå¯†é’¥
            api_url: APIç«¯ç‚¹URL
        """
        # APIè®¾ç½®
        self.api_key = api_key
        self.api_url = api_url or "https://open.bigmodel.cn/api/paas/v4/chat/completions"
        self.model = "glm-4-flash"  # é»˜è®¤ä½¿ç”¨æ›´å¿«çš„GLM-4-Flashæ¨¡å‹
        
        # æå–è®¾ç½®
        self.min_term_length = 2  # æœ¯è¯­æœ€å°é•¿åº¦
        self.max_retries = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
        
        # æ‰¹å¤„ç†ç›¸å…³æ•°æ®
        self.batch_id = None
        self.input_file_id = None
        self.output_file_id = None
        self.error_file_id = None
        
        # è¿›åº¦å›è°ƒ
        self.status_callback = None
        self.progress_callback = None
        self.complete_callback = None
        self.stop_event = None
        
        # é…ç½®
        self.config = {"output_dir": "results"}

    def set_callbacks(self, status_callback=None, progress_callback=None, complete_callback=None):
        """
        è®¾ç½®å›è°ƒå‡½æ•°
        
        Args:
            status_callback: çŠ¶æ€æ›´æ–°å›è°ƒå‡½æ•°
            progress_callback: è¿›åº¦æ›´æ–°å›è°ƒå‡½æ•°
            complete_callback: å®Œæˆå›è°ƒå‡½æ•°
        """
        self.status_callback = status_callback
        self.progress_callback = progress_callback
        self.complete_callback = complete_callback
        
    def set_status(self, status: str):
        """æ›´æ–°çŠ¶æ€"""
        if self.status_callback:
            self.status_callback(status)
            
    def update_progress(self, value: float):
        """æ›´æ–°è¿›åº¦"""
        if self.progress_callback:
            self.progress_callback(value)
            
    def notify_complete(self):
        """é€šçŸ¥å®Œæˆ"""
        if self.complete_callback:
            self.complete_callback()
            
    def stop(self):
        """åœæ­¢å¤„ç†"""
        if hasattr(self, 'stop_event') and self.stop_event:
            self.stop_event.set()
    
    def process_data(self, excel_file: str, chunks_dir: str = 'chunks', output_file: str = 'extracted_terms.xlsx') -> None:
        """å¤„ç†æ•°æ®ä¸»æ–¹æ³•"""
        if not HAS_ZHIPUAI:
            logger.error("æœªå®‰è£…zhipuaiåº“ï¼Œæ— æ³•ä½¿ç”¨æ‰¹å¤„ç†åŠŸèƒ½")
            self.set_status("é”™è¯¯: æœªå®‰è£…zhipuaiåº“ï¼Œæ— æ³•ä½¿ç”¨æ‰¹å¤„ç†åŠŸèƒ½")
            return
            
        logger.info(f"å¼€å§‹å¤„ç†æ•°æ®: {excel_file}")
        
        # æ£€æŸ¥åœæ­¢äº‹ä»¶
        if hasattr(self, 'stop_event') and self.stop_event and self.stop_event.is_set():
            logger.info("æ£€æµ‹åˆ°åœæ­¢è¯·æ±‚ï¼Œç»ˆæ­¢å¤„ç†")
            return
            
        # å¤„ç†è¾“å…¥æ•°æ®
        df, id_col, text_cols = self._process_input_data(excel_file)
        if df.empty:
            logger.error("å¤„ç†è¾“å…¥æ•°æ®å¤±è´¥")
            return
            
        # å‡†å¤‡æ‰¹å¤„ç†æ–‡ä»¶å¹¶æ‰§è¡Œ
        try:
            # åˆ›å»ºå¿…è¦çš„ç›®å½•
            os.makedirs(chunks_dir, exist_ok=True)
            
            # ç”ŸæˆJSONLæ–‡ä»¶
            jsonl_path = os.path.join(chunks_dir, f"batch_requests_{os.path.basename(excel_file)}.jsonl")
            jsonl_path = self.prepare_batch_jsonl(df, id_col, text_cols, jsonl_path)
            
            # ä¸Šä¼ æ–‡ä»¶å¹¶åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡
            self.set_status("æ­£åœ¨ä¸Šä¼ æ‰¹å¤„ç†æ–‡ä»¶...")
            input_file_id = self.upload_batch_file(jsonl_path)
            
            self.set_status("æ­£åœ¨åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡...")
            batch_id = self.create_batch_job(input_file_id, f"å¤„ç†æ–‡ä»¶: {os.path.basename(excel_file)}")
            
            # ç›‘æ§ä»»åŠ¡çŠ¶æ€
            self.set_status("æ‰¹å¤„ç†ä»»åŠ¡æ­£åœ¨è¿›è¡Œ...")
            status = self._wait_for_batch_completion(batch_id)
            
            # å¦‚æœä»»åŠ¡è¢«å–æ¶ˆæˆ–å¤±è´¥
            if not status or status['status'] != 'completed':
                return
                
            # ä¸‹è½½å’Œå¤„ç†ç»“æœ
            output_file_id = status.get('output_file_id')
            if not output_file_id:
                logger.error("æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶ID")
                self.set_status("æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶ID")
                return
                
            self.set_status("æ­£åœ¨ä¸‹è½½æ‰¹å¤„ç†ç»“æœ...")
            results_file = os.path.join(chunks_dir, f"batch_results_{batch_id}.jsonl")
            results_file = self.download_batch_results(output_file_id, results_file)
            
            self.set_status("æ­£åœ¨è§£ææ‰¹å¤„ç†ç»“æœ...")
            results = self.parse_batch_results(results_file)
            
            self.set_status("æ­£åœ¨å¯¼å‡ºç»“æœåˆ°Excel...")
            result = self.export_to_excel(results, output_file)
            
            # å®Œæˆ
            self.set_status("æ‰¹å¤„ç†å®Œæˆ")
            self.notify_complete()
            logger.info(f"æ‰¹å¤„ç†æ¨¡å¼å¤„ç†æ•°æ®å®Œæˆ: {output_file}")
            
        except Exception as e:
            logger.error(f"æ‰¹å¤„ç†è¿‡ç¨‹å‡ºé”™: {str(e)}")
            self.set_status(f"é”™è¯¯: {str(e)}")
    
    def _wait_for_batch_completion(self, batch_id: str) -> Optional[Dict]:
        """ç­‰å¾…æ‰¹å¤„ç†ä»»åŠ¡å®Œæˆ"""
        iteration = 0
        start_time = time.time()
        estimated_total_time = 300  # ä¼°è®¡éœ€è¦5åˆ†é’Ÿå®Œæˆ
        
        while True:
            # æ£€æŸ¥æ˜¯å¦è¯·æ±‚åœæ­¢
            if self.stop_event and self.stop_event.is_set():
                logger.info("æ£€æµ‹åˆ°åœæ­¢è¯·æ±‚ï¼Œç»ˆæ­¢æ‰¹å¤„ç†ç›‘æ§")
                self.set_status("ç”¨æˆ·å·²å–æ¶ˆæ“ä½œ")
                return None
                
            # æ£€æŸ¥çŠ¶æ€
            status = self.check_batch_status(batch_id)
            status_text = f"æ‰¹å¤„ç†ä»»åŠ¡çŠ¶æ€: {status['status']}..."
            
            # ä¼°è®¡è¿›åº¦å¹¶æ›´æ–°
            elapsed_time = time.time() - start_time
            if status['status'] == 'running':
                # å¦‚æœAPIè¿”å›äº†è¿›åº¦ä¿¡æ¯ï¼Œä½¿ç”¨å®ƒ
                if 'progress' in status and status['progress'] is not None:
                    progress = 0.1 + (status['progress'] * 0.8)  # 10%-90%çš„è¿›åº¦åŒºé—´
                else:
                    # åŸºäºå·²ç»ç»è¿‡çš„æ—¶é—´ä¼°è®¡è¿›åº¦
                    progress = min(0.1 + (elapsed_time / estimated_total_time * 0.8), 0.9)
                    
                # è®¡ç®—é¢„è®¡å‰©ä½™æ—¶é—´
                if progress > 0.1:
                    remaining_seconds = (elapsed_time / (progress - 0.1)) * (0.9 - progress)
                    remaining_mins = int(remaining_seconds / 60)
                    remaining_secs = int(remaining_seconds % 60)
                    status_text += f" é¢„è®¡å‰©ä½™æ—¶é—´ï¼š{remaining_mins}åˆ†{remaining_secs}ç§’"
                
                # æ›´æ–°è¿›åº¦æ¡
                if self.progress_callback:
                    self.progress_callback(progress)
            
            # æ›´æ–°çŠ¶æ€æ–‡æœ¬
            self.set_status(status_text)
                
            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
            if status['status'] == 'completed':
                # è®¾ç½®è¿›åº¦ä¸º90%ï¼Œç•™10%ç»™åç»­å¤„ç†
                if self.progress_callback:
                    self.progress_callback(0.9)
                return status
            
            # æ£€æŸ¥æ˜¯å¦å¤±è´¥
            if status['status'] in ['failed', 'canceled']:
                logger.error(f"æ‰¹å¤„ç†ä»»åŠ¡å¤±è´¥: {status.get('error', 'æœªçŸ¥é”™è¯¯')}")
                self.set_status(f"æ‰¹å¤„ç†ä»»åŠ¡å¤±è´¥: {status.get('error', 'æœªçŸ¥é”™è¯¯')}")
                return None
                
            # ç­‰å¾…ä¸€æ®µæ—¶é—´å†æ£€æŸ¥
            # å‰å‡ æ¬¡å¿«é€Ÿæ£€æŸ¥ï¼Œä¹‹åé™ä½é¢‘ç‡ï¼Œé¿å…é¢‘ç¹APIè°ƒç”¨
            if iteration < 3:
                time.sleep(5)  # å¼€å§‹æ—¶æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
            else:
                time.sleep(15)  # ä¹‹åæ¯15ç§’æ£€æŸ¥ä¸€æ¬¡
            
            iteration += 1

    def prepare_batch_jsonl(self, df: pd.DataFrame, id_col: str, text_cols: List[str], output_path: str) -> str:
        """å‡†å¤‡æ‰¹å¤„ç†JSONLæ–‡ä»¶"""
        logger.info(f"å‡†å¤‡æ‰¹å¤„ç†JSONLæ–‡ä»¶: {output_path}")
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # ç”Ÿæˆç³»ç»Ÿæç¤º
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ¯è¯­æå–ä¸“å®¶ã€‚"
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å•åˆ—æ–‡ä»¶ï¼ˆä½¿ç”¨äº†è™šæ‹ŸIDï¼‰
        is_virtual_id = id_col == "virtual_id"
        
        # åˆ›å»ºJSONLæ–‡ä»¶
        total_rows = len(df)
        with open(output_path, 'w', encoding='utf-8') as f:
            for idx, row in df.iterrows():
                # æ£€æŸ¥æ˜¯å¦è¯·æ±‚åœæ­¢
                if hasattr(self, 'stop_event') and self.stop_event and self.stop_event.is_set():
                    logger.info("æ£€æµ‹åˆ°åœæ­¢è¯·æ±‚ï¼Œç»ˆæ­¢æ‰¹å¤„ç†æ–‡ä»¶å‡†å¤‡")
                    return output_path
                    
                # è·å–è¡ŒID
                if is_virtual_id and "virtual_id" in df.columns:
                    # ä½¿ç”¨æ·»åŠ çš„è™šæ‹ŸID
                    row_id = str(row["virtual_id"])
                else:
                    # ä½¿ç”¨åŸå§‹IDåˆ—
                    row_id = str(row[id_col])
                
                # è·å–æ–‡æœ¬å†…å®¹ï¼ˆå•åˆ—æˆ–å¤šåˆ—ï¼‰
                if text_cols:
                    # ä½¿ç”¨æŒ‡å®šçš„æ–‡æœ¬åˆ—
                    text_content = " ".join([str(row[col]) for col in text_cols if col in row and pd.notna(row[col])])
                else:
                    # å¦‚æœæ²¡æœ‰æ–‡æœ¬åˆ—ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºæˆ‘ä»¬å‰é¢åšäº†æ£€æŸ¥ï¼‰
                    logger.warning(f"è¡Œ {row_id} æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬åˆ—ï¼Œå°†è·³è¿‡")
                    continue
                
                # è·³è¿‡ç©ºæ–‡æœ¬
                if not text_content or text_content.strip() == "":
                    continue
            
                # åˆ›å»ºç”¨æˆ·æç¤º
                user_prompt = self._create_extraction_prompt(text_content)
                
                # åˆ›å»ºcustom_idï¼Œç¡®ä¿ä¸è¶…è¿‡64ä¸ªå­—ç¬¦
                # ä½¿ç”¨è¡Œç´¢å¼•æ¥ç”Ÿæˆcustom_idï¼Œç¡®ä¿ä¸ä¼šè¶…é•¿
                custom_id = f"request-row{idx}"
                
                # åˆ›å»ºè¯·æ±‚ä½“
                request_body = {
                    "custom_id": custom_id,
                    "method": "POST",
                    "url": "/v4/chat/completions",
                    "body": {
                        "model": self.model,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        "temperature": 0.3,
                        "max_tokens": 2000
                    }
                }
                
                # å†™å…¥JSONL
                f.write(json.dumps(request_body, ensure_ascii=False) + '\n')
                
                # æ›´æ–°è¿›åº¦
                if self.progress_callback:
                    self.progress_callback((idx + 1) / total_rows)
                    
                # å®šæœŸæ›´æ–°çŠ¶æ€
                if idx % 100 == 0:
                    self.set_status(f"æ­£åœ¨å‡†å¤‡æ‰¹å¤„ç†æ–‡ä»¶... ({idx + 1}/{total_rows})")
        
        logger.info(f"æ‰¹å¤„ç†JSONLæ–‡ä»¶å‡†å¤‡å®Œæˆ: {output_path}")
        return output_path
        
    def _create_extraction_prompt(self, text_content: str) -> str:
        """åˆ›å»ºæœ¯è¯­æå–æç¤º"""
        return f'''# è§’è‰²ï¼šRPGæ¸¸æˆæœ¯è¯­æå–å™¨

# ä»»åŠ¡
1. ä»ä¸‹é¢ç»™å®šçš„æ¸¸æˆæ–‡æœ¬ä¸­æå–æ‰€æœ‰RPGæ¸¸æˆæœ¯è¯­å’Œä¸“æœ‰åè¯
2. æå–å¯¹è±¡ï¼šæŠ€èƒ½åç§°ã€èŒä¸šåç§°ã€ç‰©å“åç§°ã€åœ°åã€è§’è‰²åã€æ€ªç‰©åç­‰
3. æ’é™¤ä¸€èˆ¬å¸¸ç”¨è¯æ±‡å’Œéæœ¯è¯­æ€§è¡¨è¾¾
4. å¦‚æœæ–‡æœ¬æœ¬èº«å°±æ˜¯æœ¯è¯­ï¼Œä¹Ÿè¦æå–
5. ä¸¥ç¦é‡å¤æå–ç›¸åŒæœ¯è¯­
6. ä¸¥ç¦ç¼–é€ æˆ–ç”Ÿæˆä¸å­˜åœ¨äºç»™å®šæ–‡æœ¬ä¸­çš„æœ¯è¯­
7. å¿…é¡»ä¸ºæ¯ä¸ªæœ¯è¯­æä¾›ç±»å‹ä¿¡æ¯

# æœ¯è¯­ç±»å‹ç¤ºä¾‹
- æŠ€èƒ½ï¼šè¡¨ç¤ºæ¸¸æˆä¸­çš„èƒ½åŠ›ã€æ‹›å¼ã€æ³•æœ¯
- èŒä¸šï¼šè¡¨ç¤ºæ¸¸æˆä¸­çš„è§’è‰²èŒä¸šã€å®šä½
- ç‰©å“ï¼šè¡¨ç¤ºæ¸¸æˆä¸­çš„é“å…·ã€è£…å¤‡ã€æ¶ˆè€—å“
- åœ°ç‚¹ï¼šè¡¨ç¤ºæ¸¸æˆä¸­çš„åœ°åŒºã€åœºæ™¯ã€ä½ç½®
- è§’è‰²ï¼šè¡¨ç¤ºæ¸¸æˆä¸­çš„NPCã€ä¸»è§’ã€é…è§’
- æ€ªç‰©ï¼šè¡¨ç¤ºæ¸¸æˆä¸­çš„æ•Œäººã€BOSS
- ç³»ç»Ÿï¼šè¡¨ç¤ºæ¸¸æˆæœºåˆ¶ã€ç•Œé¢å…ƒç´ ã€åŠŸèƒ½
- å…¶ä»–ï¼šå¦‚æœä¸å±äºä»¥ä¸Šç±»å‹ï¼Œè¯·åˆ†ç±»ä¸ºå…¶ä»–

# è¾“å‡ºæ ¼å¼
{{"terms": [
  {{"term": "æœ¯è¯­1", "type": "ç±»å‹1", "context": "å‡ºç°ä¸Šä¸‹æ–‡"}}, 
  {{"term": "æœ¯è¯­2", "type": "ç±»å‹2", "context": "å‡ºç°ä¸Šä¸‹æ–‡"}}
]}}

# è¯´æ˜
1. åªè¾“å‡ºJSONæ ¼å¼ç»“æœï¼Œä¸è¦æ·»åŠ é¢å¤–è§£é‡Š
2. æœ¯è¯­å¿…é¡»æ¥è‡ªç»™å®šæ–‡æœ¬ï¼Œä¸è¦ç¼–é€ 
3. åªæå–æ¸¸æˆç›¸å…³çš„ä¸“ä¸šæœ¯è¯­ï¼Œä¸æå–æ™®é€šè¯æ±‡
4. å¦‚æœç»™å®šæ–‡æœ¬ä¸­æ²¡æœ‰æ¸¸æˆç›¸å…³æœ¯è¯­ï¼Œè¿”å›ç©ºæ•°ç»„: {{"terms": []}}
5. å¿…é¡»ä¸ºæ¯ä¸ªæœ¯è¯­æä¾›ä¸€ä¸ªç±»å‹ï¼Œå¦‚"æŠ€èƒ½"ã€"ç‰©å“"ã€"åœ°ç‚¹"ç­‰

# éœ€è¦æå–çš„æ–‡æœ¬ï¼š
{text_content}'''

    def upload_batch_file(self, file_path: str) -> str:
        """
        ä¸Šä¼ æ‰¹å¤„ç†æ–‡ä»¶å¹¶è·å–æ–‡ä»¶ID
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ä»¶ID
        """
        if not HAS_ZHIPUAI:
            raise ImportError("æœªå®‰è£…zhipuaiåº“ï¼Œæ— æ³•ä½¿ç”¨æ‰¹å¤„ç†åŠŸèƒ½")
            
        logger.info(f"ä¸Šä¼ æ‰¹å¤„ç†æ–‡ä»¶: {file_path}")
        
        client = ZhipuAI(api_key=self.api_key.strip())
        
        try:
            with open(file_path, "rb") as f:
                result = client.files.create(
                    file=f,
                    purpose="batch"
                )
            
            file_id = result.id
            logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼ŒID: {file_id}")
            return file_id
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")
            raise
            
    def create_batch_job(self, input_file_id: str, description: str = "Term Extraction") -> str:
        """
        åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡
        
        Args:
            input_file_id: è¾“å…¥æ–‡ä»¶ID
            description: ä»»åŠ¡æè¿°
            
        Returns:
            æ‰¹å¤„ç†ä»»åŠ¡ID
        """
        if not HAS_ZHIPUAI:
            raise ImportError("æœªå®‰è£…zhipuaiåº“ï¼Œæ— æ³•ä½¿ç”¨æ‰¹å¤„ç†åŠŸèƒ½")
            
        logger.info(f"åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡ï¼Œè¾“å…¥æ–‡ä»¶ID: {input_file_id}")
        
        client = ZhipuAI(api_key=self.api_key.strip())
        
        try:
            result = client.batches.create(
                input_file_id=input_file_id,
                endpoint="/v4/chat/completions",
                completion_window="24h",  # æ·»åŠ 24å°æ—¶çš„å®Œæˆçª—å£
                metadata={
                    "description": description
                }
            )
            
            batch_id = result.id
            logger.info(f"æ‰¹å¤„ç†ä»»åŠ¡åˆ›å»ºæˆåŠŸï¼ŒID: {batch_id}")
            self.batch_id = batch_id
            self.input_file_id = input_file_id
            return batch_id
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡å¤±è´¥: {str(e)}")
            raise

    def check_batch_status(self, batch_id: Optional[str] = None) -> Dict:
        """
        æ£€æŸ¥æ‰¹å¤„ç†ä»»åŠ¡çŠ¶æ€
        
        Args:
            batch_id: æ‰¹å¤„ç†ä»»åŠ¡IDï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰ä»»åŠ¡ID
            
        Returns:
            ä»»åŠ¡çŠ¶æ€ä¿¡æ¯
        """
        if not HAS_ZHIPUAI:
            raise ImportError("æœªå®‰è£…zhipuaiåº“ï¼Œæ— æ³•ä½¿ç”¨æ‰¹å¤„ç†åŠŸèƒ½")
            
        batch_id = batch_id or self.batch_id
        if not batch_id:
            raise ValueError("æœªæŒ‡å®šæ‰¹å¤„ç†ä»»åŠ¡ID")
            
        logger.info(f"æ£€æŸ¥æ‰¹å¤„ç†ä»»åŠ¡çŠ¶æ€ï¼ŒID: {batch_id}")
        
        client = ZhipuAI(api_key=self.api_key.strip())
        
        try:
            result = client.batches.retrieve(batch_id)
            
            # ä¿å­˜è¾“å‡ºå’Œé”™è¯¯æ–‡ä»¶ID
            if hasattr(result, 'output_file_id'):
                self.output_file_id = result.output_file_id
            if hasattr(result, 'error_file_id'):
                self.error_file_id = result.error_file_id
                
            # è½¬æ¢ä¸ºå­—å…¸
            status_dict = {
                'id': result.id,
                'status': result.status,
                'created_at': result.created_at,
                'expires_at': result.expires_at if hasattr(result, 'expires_at') else None,
                'error': result.error if hasattr(result, 'error') else None,
                'output_file_id': result.output_file_id if hasattr(result, 'output_file_id') else None,
                'error_file_id': result.error_file_id if hasattr(result, 'error_file_id') else None
            }
            
            logger.info(f"æ‰¹å¤„ç†ä»»åŠ¡çŠ¶æ€: {status_dict['status']}")
            return status_dict
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ‰¹å¤„ç†ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")
            raise

    def download_batch_results(self, file_id: Optional[str] = None, output_path: str = None) -> str:
        """
        ä¸‹è½½æ‰¹å¤„ç†ç»“æœ
        
        Args:
            file_id: æ–‡ä»¶IDï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰è¾“å‡ºæ–‡ä»¶ID
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not HAS_ZHIPUAI:
            raise ImportError("æœªå®‰è£…zhipuaiåº“ï¼Œæ— æ³•ä½¿ç”¨æ‰¹å¤„ç†åŠŸèƒ½")
            
        file_id = file_id or self.output_file_id
        if not file_id:
            raise ValueError("æœªæŒ‡å®šæ–‡ä»¶ID")
            
        if not output_path:
            output_path = os.path.join(os.getcwd(), f"batch_results_{file_id}.jsonl")
            
        logger.info(f"ä¸‹è½½æ‰¹å¤„ç†ç»“æœï¼Œæ–‡ä»¶ID: {file_id}ï¼Œè¾“å‡ºè·¯å¾„: {output_path}")
        
        client = ZhipuAI(api_key=self.api_key.strip())
        
        try:
            content = client.files.content(file_id)
            
            # å†™å…¥æ–‡ä»¶
            content.write_to_file(output_path)
            
            logger.info(f"æ‰¹å¤„ç†ç»“æœä¸‹è½½æˆåŠŸ: {output_path}")
            return output_path
                
        except Exception as e:
            logger.error(f"ä¸‹è½½æ‰¹å¤„ç†ç»“æœå¤±è´¥: {str(e)}")
            raise

    def parse_batch_results(self, results_file: str) -> List[Dict]:
        """
        è§£ææ‰¹å¤„ç†ç»“æœæ–‡ä»¶
        
        Args:
            results_file: ç»“æœæ–‡ä»¶è·¯å¾„
            
        Returns:
            è§£æåçš„ç»“æœåˆ—è¡¨
        """
        logger.info(f"è§£ææ‰¹å¤„ç†ç»“æœæ–‡ä»¶: {results_file}")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†è™šæ‹ŸIDï¼ˆé€šè¿‡æ–‡ä»¶ååˆ¤æ–­ï¼‰
        is_virtual_id = "virtual_id" in results_file
        
        results = []
        
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        # è§£ææ¯ä¸€è¡Œ
                        data = json.loads(line)
                        
                        # æå–è¯·æ±‚IDå’Œå“åº”å†…å®¹
                        custom_id = data.get('custom_id', '')
                        
                        # ä»custom_idä¸­è§£ærow_id
                        row_id = "unknown"
                        
                        # å°è¯•å¤„ç†å„ç§å¯èƒ½çš„è‡ªå®šä¹‰IDæ ¼å¼
                        if custom_id.startswith("request-row"):
                            # å¤„ç†æ ¼å¼ "request-row{idx}"
                            row_id = custom_id.replace("request-row", "")
                        elif '-' in custom_id and custom_id.split('-')[0].isdigit():
                            # å¤„ç†æ ¼å¼ "row_id-truncated"
                            row_id = custom_id.split('-')[0]
                        else:
                            # å…¶ä»–æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨custom_id
                            row_id = custom_id
                            
                        # ç¡®ä¿row_idæ˜¯å­—ç¬¦ä¸²
                        row_id = str(row_id)
                        
                        # è·å–å“åº”å†…å®¹
                        response_data = data.get('response', {})
                        body = response_data.get('body', {})
                        choices = body.get('choices', [])
                        
                        if choices and len(choices) > 0:
                            content = choices[0].get('message', {}).get('content', '')
                            
                            # å¤„ç†å“åº”å†…å®¹
                            extracted_terms = self._process_api_response(content)
                            
                            # æ·»åŠ è¡ŒIDåˆ°æ¯ä¸ªæœ¯è¯­
                            for term in extracted_terms:
                                term['row_id'] = row_id
                                
                            # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                            results.extend(extracted_terms)
                    except Exception as e:
                        logger.warning(f"è§£ææ‰¹å¤„ç†ç»“æœè¡Œå¤±è´¥: {str(e)}")
                        continue
                        
            # ä¿å­˜è§£æç»“æœåˆ°JSONæ–‡ä»¶
            if self.batch_id:
                try:
                    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                    output_dir = os.path.join(self.config.get('output_dir', 'results'), self.batch_id)
                    os.makedirs(output_dir, exist_ok=True)
                    
                    # ä¿å­˜è§£æç»“æœ
                    results_path = os.path.join(output_dir, f"{self.batch_id}_parsed_results.json")
                    with open(results_path, 'w', encoding='utf-8') as f:
                        json.dump(results, f, ensure_ascii=False, indent=2)
                        
                    logger.info(f"è§£æç»“æœå·²ä¿å­˜åˆ°: {results_path}")
                except Exception as e:
                    logger.error(f"ä¿å­˜è§£æç»“æœæ—¶å‡ºé”™: {str(e)}")
                    
        except Exception as e:
            logger.error(f"è§£ææ‰¹å¤„ç†ç»“æœæ–‡ä»¶å¤±è´¥: {str(e)}")
            raise
            
        logger.info(f"æ‰¹å¤„ç†ç»“æœè§£æå®Œæˆï¼Œå…±æå– {len(results)} ä¸ªæœ¯è¯­")
        return results
    
    def _process_api_response(self, content: str) -> List[Dict[str, str]]:
        """å¤„ç†APIå“åº”å†…å®¹"""
        if not content or content.strip() == "":
            logger.warning("âš ï¸ æ”¶åˆ°ç©ºå“åº”")
            return []
        
        logger.debug(f"ğŸ“ åŸå§‹å“åº”å†…å®¹: {content[:100]}...")
        
        # å°è¯•å¤šç§æ–¹æ³•è§£æJSON
        result = None
        
        # æ–¹æ³•1: ä½¿ç”¨é«˜çº§JSONå¤„ç†å·¥å…·
        if HAS_JSON_UTILS:
            try:
                result = fix_json_response(content, expected_keys=["terms"])
            except Exception:
                pass
                
        # æ–¹æ³•2: æ ‡å‡†JSONè§£æ
        if result is None:
            try:
                # æ¸…ç†å†…å®¹
                cleaned_content = re.sub(r'```json|\s*```', '', content)
                cleaned_content = re.sub(r'//.*?$', '', cleaned_content, flags=re.MULTILINE)
                result = json.loads(cleaned_content)
            except json.JSONDecodeError:
                pass
        
        # æ–¹æ³•3: æ­£åˆ™è¡¨è¾¾å¼æå–
        if result is None:
            try:
                json_pattern = r'\{[^{}]*(((?R)[^{}]*)*)\}|\[[^\[\]]*(((?R)[^\[\]]*)*)\]'
                for match in re.finditer(json_pattern, content, re.DOTALL):
                    try:
                        result = json.loads(match.group(0))
                        break
                    except:
                        continue
            except:
                pass
        
        # å¤„ç†ç»“æœ
        if result is None:
            logger.warning("âš ï¸ æ‰€æœ‰è§£ææ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []
            
        # æå–æœ¯è¯­åˆ—è¡¨
        terms = []
        if "terms" in result:
            terms = result["terms"]
        elif isinstance(result, list):
            terms = result
            
        # è§„èŒƒåŒ–æœ¯è¯­
        return [self._normalize_term(term) for term in terms if term]
    
    def _normalize_term(self, term_data: Any) -> Dict[str, str]:
        """è§„èŒƒåŒ–æœ¯è¯­æ•°æ®"""
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ›å»ºåŸºæœ¬æœ¯è¯­å¯¹è±¡
        if isinstance(term_data, str):
            return {"term": term_data}
            
        # å¦‚æœæ—¢ä¸æ˜¯å­—ç¬¦ä¸²ä¹Ÿä¸æ˜¯å­—å…¸ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if not isinstance(term_data, dict):
            try:
                return {"term": str(term_data)}
            except:
                return {"term": "æœªçŸ¥æœ¯è¯­"}
        
        # è§„èŒƒåŒ–å­—æ®µ
        normalized = {}
        
        # å­—æ®µæ˜ å°„è¡¨
        field_mappings = {
            "term": ["term", "æœ¯è¯­", "name", "åç§°", "æœ¯è¯­åç§°", "æœ¯è¯­å"],
            "type": ["type", "ç±»å‹", "æœ¯è¯­ç±»å‹"],
            "context": ["context", "ä¸Šä¸‹æ–‡", "å¥å­", "sentence"]
        }
        
        # æŸ¥æ‰¾å’Œæ˜ å°„å­—æ®µ
        for target_field, possible_keys in field_mappings.items():
            for key in possible_keys:
                if key in term_data and term_data[key]:
                    normalized[target_field] = str(term_data[key])
                    break
        
        # ç¡®ä¿è‡³å°‘æœ‰æœ¯è¯­å­—æ®µ
        if "term" not in normalized:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ¯è¯­å­—æ®µï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªéç©ºå­—æ®µ
            for key, value in term_data.items():
                if value:
                    normalized["term"] = str(value)
                    break
            
            # å¦‚æœä»ç„¶æ²¡æœ‰æœ¯è¯­å­—æ®µï¼Œä½¿ç”¨é»˜è®¤å€¼
            if "term" not in normalized:
                normalized["term"] = "æœªçŸ¥æœ¯è¯­"
            
        return normalized

    def _process_input_data(self, excel_file: str) -> Tuple[pd.DataFrame, str, List[str]]:
        """
        å¤„ç†è¾“å…¥Excelæ•°æ®
        
        Args:
            excel_file: Excelæ–‡ä»¶è·¯å¾„
            
        Returns:
            DataFrame, IDåˆ—, æ–‡æœ¬åˆ—åˆ—è¡¨
        """
        # è¯»å–Excelæ–‡ä»¶
        df = self.read_excel(excel_file)
        if df.empty:
            logger.error("Excelæ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")
            return pd.DataFrame(), "", []
            
        # è¯†åˆ«åˆ—
        id_col, text_cols = self.identify_columns(df)
        
        # æ£€æµ‹è™šæ‹ŸID - å•åˆ—Excelæ–‡ä»¶çš„ç‰¹æ®Šå¤„ç†
        if len(df.columns) == 1:
            logger.info("å¤„ç†å•åˆ—Excelæ–‡ä»¶ - å°†æ·»åŠ ç´¢å¼•ä½œä¸ºè™šæ‹ŸID")
            # æ·»åŠ ä¸€ä¸ªè™šæ‹ŸIDåˆ—ï¼Œä½¿ç”¨æ•°æ®æ¡†çš„ç´¢å¼•ä½œä¸ºIDå€¼
            df["virtual_id"] = df.index.astype(str)
            id_col = "virtual_id" if not id_col else id_col
            
        # æ£€æŸ¥æ–‡æœ¬åˆ—æ˜¯å¦å­˜åœ¨    
        if not text_cols:
            logger.error("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æœ¬åˆ—")
            return pd.DataFrame(), "", []
            
        logger.info(f"è¯†åˆ«åˆ°IDåˆ—: {id_col}, æ–‡æœ¬åˆ—: {', '.join(text_cols)}")
        
        return df, id_col, text_cols
        
    def read_excel(self, file_path: str) -> pd.DataFrame:
        """
        è¯»å–Excelæ–‡ä»¶ï¼Œè¿”å›DataFrame
        
        Args:
            file_path: Excelæ–‡ä»¶è·¯å¾„
            
        Returns:
            DataFrame
        """
        try:
            logger.info(f"è¯»å–Excelæ–‡ä»¶: {file_path}")
            
            # æ£€æµ‹æ–‡ä»¶ç±»å‹
            if file_path.lower().endswith('.csv'):
                df = pd.read_csv(file_path, encoding='utf-8-sig')
            else:
                df = pd.read_excel(file_path)
            
            if df.empty:
                logger.warning(f"Excelæ–‡ä»¶ä¸ºç©º: {file_path}")
            else:
                logger.info(f"æˆåŠŸè¯»å–æ–‡ä»¶: {len(df)}è¡Œ x {len(df.columns)}åˆ—")
                
            return df
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def identify_columns(self, df: pd.DataFrame, id_col: Optional[str] = None) -> Tuple[str, List[str]]:
        """
        è¯†åˆ«Excelä¸­çš„IDåˆ—å’Œæ–‡æœ¬åˆ—
        
        Args:
            df: Excelæ•°æ®DataFrame
            id_col: æŒ‡å®šçš„IDåˆ—åï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨è¯†åˆ«
            
        Returns:
            (IDåˆ—å, æ–‡æœ¬åˆ—ååˆ—è¡¨)
        """
        if df.empty:
            return "", []
            
        # ç¡®ä¿æ‰€æœ‰åˆ—åéƒ½æ˜¯å­—ç¬¦ä¸²
        df.columns = df.columns.astype(str)
        
        # ç‰¹æ®Šå¤„ç†ï¼šå•åˆ—Excelæ–‡ä»¶
        if len(df.columns) == 1:
            logger.info("æ£€æµ‹åˆ°å•åˆ—Excelæ–‡ä»¶")
            # å¯¹äºå•åˆ—æ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ªè™šæ‹ŸIDåˆ—ï¼Œå°†å”¯ä¸€åˆ—ä½œä¸ºæ–‡æœ¬åˆ—
            column_name = df.columns[0]
            logger.info(f"å•åˆ—æ–‡ä»¶ï¼šå°†å”¯ä¸€åˆ— {column_name} ä½œä¸ºæ–‡æœ¬åˆ—å¤„ç†")
            # è¿”å›ä¸€ä¸ªè™šæ‹ŸIDåˆ—åå’Œå®é™…çš„æ–‡æœ¬åˆ—å
            return "virtual_id", [column_name]
        
        # å¦‚æœæŒ‡å®šäº†IDåˆ—ï¼ŒéªŒè¯å®ƒæ˜¯å¦å­˜åœ¨
        if id_col is not None and id_col in df.columns:
            identified_id_col = id_col
        else:
            # è‡ªåŠ¨è¯†åˆ«IDåˆ—
            # ä¼˜å…ˆçº§ï¼š(1) åŒ…å«"id"çš„åˆ—å (2) ç¬¬ä¸€åˆ—
            id_candidates = [col for col in df.columns if 'id' in col.lower()]
            
            if id_candidates:
                identified_id_col = id_candidates[0]
            else:
                # ä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºIDåˆ—
                identified_id_col = df.columns[0]
        
        # è¯†åˆ«æ–‡æœ¬åˆ—ï¼šé™¤IDåˆ—å¤–çš„æ‰€æœ‰åˆ—
        text_cols = [col for col in df.columns if col != identified_id_col]
        
        # è¿‡æ»¤æ‰å¯èƒ½ä¸åŒ…å«æœ‰ç”¨æ–‡æœ¬çš„åˆ—
        useful_text_cols = []
        for col in text_cols:
            # æ£€æŸ¥åˆ—æ˜¯å¦ä¸ºå¯èƒ½çš„æ–‡æœ¬åˆ—
            sample = df[col].dropna().astype(str).iloc[:10] if len(df) > 10 else df[col].dropna().astype(str)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ–‡æœ¬å†…å®¹
            if any(len(str(text)) > 5 for text in sample):
                useful_text_cols.append(col)
        
        if not useful_text_cols:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰ç”¨çš„æ–‡æœ¬åˆ—ï¼Œé€€å›åˆ°ä½¿ç”¨æ‰€æœ‰éIDåˆ—
            logger.warning("æœªæ‰¾åˆ°æœ‰ç”¨çš„æ–‡æœ¬åˆ—ï¼Œå°†ä½¿ç”¨æ‰€æœ‰éIDåˆ—")
            useful_text_cols = text_cols
            
        logger.info(f"è¯†åˆ«åˆ°IDåˆ—: {identified_id_col}")
        logger.info(f"è¯†åˆ«åˆ°æ–‡æœ¬åˆ—: {', '.join(useful_text_cols)}")
        
        return identified_id_col, useful_text_cols

    def export_to_excel(self, results: List[Dict], output_file: str) -> Dict[str, Any]:
        """å°†æå–çš„æœ¯è¯­å¯¼å‡ºåˆ°Excelæ–‡ä»¶"""
        logger.info(f"å¯¼å‡º {len(results)} ä¸ªæœ¯è¯­åˆ°: {output_file}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_file)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        # åˆ›å»ºè¾“å‡ºDataFrame
        df = pd.DataFrame(results)
        
        # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´
        required_columns = ["row_id", "term", "type", "context"]
        for col in required_columns:
            if col not in df.columns:
                df[col] = ""
        
        # é‡æ–°æ’åºåˆ—
        column_order = ["row_id"] + [col for col in df.columns if col != "row_id"]
        df = df[column_order]
        
        try:
            # å°è¯•ä¿å­˜ä¸ºExcelæ ¼å¼
            logger.info(f"å¯¼å‡ºä¸ºExcelæ ¼å¼: {output_file}")
            df.to_excel(output_file, index=False)
            logger.info(f"æˆåŠŸå¯¼å‡ºåˆ°Excel: {output_file}")
            return {"success": True, "message": "å¯¼å‡ºExcelæˆåŠŸ", "output_file": output_file}
                
        except Exception as e:
            logger.warning(f"å¯¼å‡ºExcelæ ¼å¼å¤±è´¥ï¼Œå°è¯•CSVæ ¼å¼: {str(e)}")
            
            # å°è¯•CSVæ ¼å¼
            try:
                csv_file = output_file.replace('.xlsx', '.csv')
                df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                logger.info(f"æˆåŠŸå¯¼å‡ºä¸ºCSV: {csv_file}")
                return {"success": True, "message": "å¯¼å‡ºä¸ºCSVæ ¼å¼", "output_file": csv_file}
            except Exception as csv_e:
                logger.error(f"å¯¼å‡ºç»“æœå¤±è´¥: {str(csv_e)}")
                return {"success": False, "message": f"å¯¼å‡ºå¤±è´¥: {str(e)}", "output_file": ""}

    def test_api_key(self) -> bool:
        """
        æµ‹è¯•APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ
        
        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        if not self.api_key:
            logger.error("APIå¯†é’¥ä¸ºç©º")
            return False
            
        if not HAS_ZHIPUAI:
            logger.error("æœªå®‰è£…zhipuaiåº“ï¼Œæ— æ³•æµ‹è¯•APIå¯†é’¥")
            return False
            
        try:
            # ä½¿ç”¨zhipuaiåº“æµ‹è¯•
            client = ZhipuAI(api_key=self.api_key.strip())
            
            # å‘é€ç®€å•çš„è¯·æ±‚
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": "ä½ å¥½"}
                ],
                max_tokens=10
            )
            
            logger.info("APIå¯†é’¥æµ‹è¯•æˆåŠŸ")
            return True
                
        except Exception as e:
            logger.error(f"APIå¯†é’¥æµ‹è¯•å¤±è´¥: {str(e)}")
            return False

    def _extract_via_zhipuai(self, prompt, model=None):
        """ä½¿ç”¨æ™ºè°±AIæ‰¹é‡å¤„ç†æ¥å£è¿›è¡Œæå–"""
        try:
            client = ZhipuAI(api_key=self.api_key)
            response = client.chat.completions.create(
                model=model or self.model,
                temperature=0.3,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ¯è¯­æå–ä¸“å®¶ï¼Œæ“…é•¿ä»æ–‡æœ¬ä¸­æå–ä¸“ä¸šæœ¯è¯­å’Œä¸“æœ‰åè¯ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000  # ä½¿ç”¨å›ºå®šçš„max_tokenså€¼
            )
            return response.choices[0].message.content
        except Exception as e:
            logging.error(f"æ™ºè°±APIè°ƒç”¨å¤±è´¥: {str(e)}")
            return None

    def process_file(self, file_path, id_column=None, text_column=None) -> Tuple[pd.DataFrame, str]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œæå–æœ¯è¯­å¹¶è¿”å›ç»“æœ
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            id_column: IDåˆ—å
            text_column: æ–‡æœ¬åˆ—å
            
        Returns:
            å…ƒç»„(æå–çš„æœ¯è¯­DataFrame, ç»“æœç›®å½•)
        """
        # ç”Ÿæˆå”¯ä¸€çš„æ‰¹æ¬¡IDï¼ŒåŸºäºæ–‡ä»¶åå’Œæ—¶é—´æˆ³
        filename = os.path.basename(file_path)
        self.batch_id = f"{os.path.splitext(filename)[0]}_{int(time.time())}"
        logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶ {file_path}, æ‰¹æ¬¡ID: {self.batch_id}")
        
        try:
            # è¯»å–Excelæˆ–CSVæ–‡ä»¶
            if file_path.endswith('.csv'):
                df = pd.read_csv(file_path)
            else:
                df = pd.read_excel(file_path)
                
            logger.info(f"æˆåŠŸè¯»å–æ–‡ä»¶ï¼ŒåŒ…å« {len(df)} è¡Œæ•°æ®ï¼Œåˆ—: {df.columns.tolist()}")
            
            is_virtual_id = False
            
            # æ£€æŸ¥æ˜¯å¦åªæœ‰ä¸€åˆ—
            single_column = len(df.columns) == 1
            
            # å¦‚æœåªæœ‰ä¸€åˆ—ä¸”æ²¡æœ‰æŒ‡å®šåˆ—ï¼Œå°†å…¶è§†ä¸ºæ–‡æœ¬åˆ—å¹¶åˆ›å»ºè™šæ‹ŸID
            if single_column:
                text_column = df.columns[0]
                if id_column is None:
                    logger.info(f"å•åˆ—æ–‡ä»¶æ£€æµ‹åˆ°ï¼Œå°†ä½¿ç”¨è™šæ‹ŸIDå¹¶å°†åˆ— '{text_column}' è®¾ä¸ºæ–‡æœ¬åˆ—")
                    # åˆ›å»ºè™šæ‹ŸIDåˆ—
                    df['virtual_id'] = [f"row_{i}" for i in range(len(df))]
                    id_column = 'virtual_id'
                    is_virtual_id = True
                else:
                    # å¦‚æœç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†IDåˆ—ï¼Œæˆ‘ä»¬éœ€è¦æ£€æŸ¥å®ƒæ˜¯å¦æ˜¯å”¯ä¸€åˆ—
                    if id_column == df.columns[0]:
                        logger.warning(f"å”¯ä¸€çš„åˆ— '{id_column}' è¢«æŒ‡å®šä¸ºIDåˆ—ï¼Œä½†æ²¡æœ‰æ–‡æœ¬åˆ—å¯ç”¨")
                        logger.info(f"å°†åˆ›å»ºè™šæ‹Ÿæ–‡æœ¬åˆ—ï¼Œå¤åˆ¶IDåˆ—çš„å†…å®¹")
                        # åˆ›å»ºè™šæ‹Ÿæ–‡æœ¬åˆ—
                        df['virtual_text'] = df[id_column].copy()
                        text_column = 'virtual_text'
                    
            # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
            if id_column and id_column not in df.columns:
                if id_column == 'virtual_id':
                    logger.info("ä½¿ç”¨è™šæ‹ŸIDåˆ—")
                    df['virtual_id'] = [f"row_{i}" for i in range(len(df))]
                    is_virtual_id = True
                else:
                    logger.error(f"IDåˆ— '{id_column}' åœ¨æ–‡ä»¶ä¸­ä¸å­˜åœ¨")
                    raise ValueError(f"IDåˆ— '{id_column}' åœ¨æ–‡ä»¶ä¸­ä¸å­˜åœ¨")
                    
            if text_column and text_column not in df.columns:
                logger.error(f"æ–‡æœ¬åˆ— '{text_column}' åœ¨æ–‡ä»¶ä¸­ä¸å­˜åœ¨")
                raise ValueError(f"æ–‡æœ¬åˆ— '{text_column}' åœ¨æ–‡ä»¶ä¸­ä¸å­˜åœ¨")
                
            # å¦‚æœæ²¡æœ‰æŒ‡å®šåˆ—ï¼Œå°è¯•çŒœæµ‹
            if not id_column:
                potential_id_cols = [col for col in df.columns if 'id' in col.lower()]
                if potential_id_cols:
                    id_column = potential_id_cols[0]
                    logger.info(f"è‡ªåŠ¨é€‰æ‹© '{id_column}' ä¸ºIDåˆ—")
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°IDåˆ—ï¼Œåˆ›å»ºè™šæ‹ŸID
                    logger.info("æ— æ³•è¯†åˆ«IDåˆ—ï¼Œå°†åˆ›å»ºè™šæ‹ŸID")
                    df['virtual_id'] = [f"row_{i}" for i in range(len(df))]
                    id_column = 'virtual_id'
                    is_virtual_id = True
                    
            if not text_column:
                # æ’é™¤IDåˆ—ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªéIDåˆ—ä½œä¸ºæ–‡æœ¬åˆ—
                text_candidates = [col for col in df.columns if col != id_column]
                if text_candidates:
                    text_column = text_candidates[0]
                    logger.info(f"è‡ªåŠ¨é€‰æ‹© '{text_column}' ä¸ºæ–‡æœ¬åˆ—")
                else:
                    logger.error("æ— æ³•è¯†åˆ«æ–‡æœ¬åˆ—ï¼Œè¯·æ˜ç¡®æŒ‡å®š")
                    raise ValueError("æ— æ³•è¯†åˆ«æ–‡æœ¬åˆ—ï¼Œè¯·æ˜ç¡®æŒ‡å®š")
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = os.path.join(self.config.get('output_dir', 'results'), self.batch_id)
            os.makedirs(output_dir, exist_ok=True)
            
            # å‡†å¤‡æ‰¹å¤„ç†æ•°æ®
            batch_data = []
            for idx, row in df.iterrows():
                row_id = str(row[id_column])
                text = str(row[text_column])
                
                # æ¸…ç†æ–‡æœ¬
                if text:
                    text = re.sub(r'\s+', ' ', text).strip()
                    
                if not text:
                    logger.warning(f"è¡Œ {idx} (ID: {row_id}) æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                    
                # å‡†å¤‡è¯·æ±‚æ•°æ®
                custom_id = f"{row_id}"
                if len(custom_id) > 60:  # APIå¯èƒ½å¯¹IDé•¿åº¦æœ‰é™åˆ¶
                    custom_id = f"{row_id[:25]}-{row_id[-25:]}"
                    
                prompt = self._create_extraction_prompt(text)
                
                batch_data.append({
                    "custom_id": custom_id,
                    "prompt": prompt
                })
                
            logger.info(f"å‡†å¤‡äº† {len(batch_data)} æ¡è®°å½•è¿›è¡Œå¤„ç†")
            
            # ä¿å­˜æ‰¹å¤„ç†è¾“å…¥æ•°æ®
            batch_input_file = os.path.join(output_dir, f"{self.batch_id}_input.json")
            with open(batch_input_file, 'w', encoding='utf-8') as f:
                json.dump(batch_data, f, ensure_ascii=False, indent=2)
            logger.info(f"æ‰¹å¤„ç†è¾“å…¥æ•°æ®å·²ä¿å­˜åˆ°: {batch_input_file}")
            
            # ä¸Šä¼ æ‰¹å¤„ç†æ–‡ä»¶å¹¶è·å–ç»“æœ
            results_file = self.upload_batch_file(batch_input_file)
            
            if not results_file:
                logger.error("æ‰¹å¤„ç†å¤±è´¥ï¼Œæœªè¿”å›ç»“æœæ–‡ä»¶")
                return pd.DataFrame(), output_dir
                
            # è§£æç»“æœ
            parsed_results = self.parse_batch_results(results_file)
            
            if not parsed_results:
                logger.error("è§£ææ‰¹å¤„ç†ç»“æœå¤±è´¥")
                return pd.DataFrame(), output_dir
                
            # åˆ›å»ºç»“æœDataFrame
            result_data = []
            for term_info in parsed_results:
                row_id = term_info.get('row_id', 'unknown')
                
                # æŸ¥æ‰¾åŸå§‹æ–‡æœ¬
                original_text = ""
                if is_virtual_id:
                    # å¯¹äºè™šæ‹ŸIDï¼Œå°è¯•ä»row_idä¸­æå–è¡Œå·
                    try:
                        if row_id.startswith('row_'):
                            row_idx = int(row_id.replace('row_', ''))
                            if 0 <= row_idx < len(df):
                                original_text = df.iloc[row_idx][text_column]
                    except:
                        pass
                else:
                    # å¯¹äºçœŸå®IDï¼Œé€šè¿‡IDæŸ¥æ‰¾æ–‡æœ¬
                    try:
                        matches = df[df[id_column] == row_id]
                        if not matches.empty:
                            original_text = matches.iloc[0][text_column]
                    except:
                        pass
                
                result_data.append({
                    'row_id': row_id,
                    'original_text': original_text,
                    'term': term_info.get('term', ''),
                    'normalized_term': term_info.get('normalized', ''),
                    'pos': term_info.get('pos', ''),
                    'frequency': term_info.get('frequency', 1),
                    'score': term_info.get('score', 0.0)
                })
                
            results_df = pd.DataFrame(result_data)
            
            # å¯¼å‡ºç»“æœåˆ°Excel
            excel_file = os.path.join(output_dir, f"{self.batch_id}_results.xlsx")
            self.export_to_excel(results_df, excel_file)
            
            logger.info(f"æ–‡ä»¶å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {excel_file}")
            return results_df, output_dir
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            logger.exception(e)
            raise

def main():
    """ä¸»å‡½æ•°"""
    try:
        import argparse
        parser = argparse.ArgumentParser(description='æœ¯è¯­æå–å·¥å…· - æ‰¹å¤„ç†æ¨¡å¼')
        parser.add_argument('--input', help='è¾“å…¥Excelæ–‡ä»¶è·¯å¾„')
        parser.add_argument('--output', help='è¾“å‡ºExcelæ–‡ä»¶è·¯å¾„')
        parser.add_argument('--api-key', help='æ™ºè°±APIå¯†é’¥')
        args = parser.parse_args()
        
        # æ‰“å°å½“å‰å·¥ä½œç›®å½•å’Œå‚æ•°
        logger.info(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        logger.info(f"è¾“å…¥æ–‡ä»¶: {args.input}")
        logger.info(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
        
        # æ£€æŸ¥zhipuaiåº“æ˜¯å¦å·²å®‰è£…
        if not HAS_ZHIPUAI:
            logger.error("æœªå®‰è£…zhipuaiåº“ï¼Œæ— æ³•ä½¿ç”¨æ‰¹å¤„ç†åŠŸèƒ½")
            print("é”™è¯¯: æœªå®‰è£…zhipuaiåº“ã€‚è¯·ä½¿ç”¨ 'pip install zhipuai' å®‰è£…")
            return
        
        # å¦‚æœæœªæŒ‡å®šè¾“å…¥æ–‡ä»¶ï¼Œæç¤ºå¹¶é€€å‡º
        if not args.input:
            print("è¯·ä½¿ç”¨--inputå‚æ•°æŒ‡å®šè¾“å…¥Excelæ–‡ä»¶")
            return
            
        # è®¾ç½®é»˜è®¤è¾“å‡ºæ–‡ä»¶
        output_file = args.output or "extracted_terms.xlsx"
        
        # è·å–APIå¯†é’¥
        api_key = args.api_key
        if not api_key:
            api_key = input("è¯·è¾“å…¥æ™ºè°±APIå¯†é’¥: ")
            
        # åˆ›å»ºæœ¯è¯­æå–å™¨
        extractor = TermExtractor(api_key=api_key)
    
    # å¤„ç†æ•°æ®
        extractor.process_data(args.input, output_file=output_file)
        
        print(f"æœ¯è¯­æå–å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        logger.error(f"è¿è¡Œå‡ºé”™: {str(e)}")
        print(f"è¿è¡Œå‡ºé”™: {str(e)}")


if __name__ == "__main__":
    main() 